# Novelty Hunt Results — 2026-02-28 14:23

Hunt argument: "for anything novel (either conceptually, architecturally, scientifically, or otherwise) that we could implement to improve this system"

Model state at time of hunt: **v23** (held-out_r=0.696) | DB: 22,186 texts / 90,361 scores / 34,850 sep-llm

---

## Ranked Novel Items

### 1. Contrastive Decorrelation Loss *(architectural)*

**What:** Add a pairwise anti-correlation regularizer to `compute_loss()` in `distill.py`. Currently the loss is MSE + confidence + optional g-head — nothing pushes dimension outputs apart. Mean inter-dim |r|=0.632 is the core structural barrier; it's why g-factor explains 67.3% of variance and why single-number g-PSQ is near-chance on criterion tasks.

**Mechanism:** Barlow Twins-style penalty — compute the cross-correlation matrix of the model's 10 output dimensions over each training batch, penalize off-diagonal entries proportional to their squared magnitude. This is a soft regularizer: it nudges gradients toward independent outputs without forcing orthogonality. Coefficient is a tunable hyperparameter (start at 0.01).

**Why novel:** No prior psychometric distillation work uses contrastive decorrelation. It directly attacks the root cause of g-factor dominance — not by adding data or changing the scoring protocol, but by modifying the loss geometry.

**Where:** `scripts/distill.py` → `compute_loss()` (~30 new lines + hyperparameter flag `--decorr-weight`)

**Expected impact:** If inter-dim |r| drops from 0.632 to ~0.50, dimension-specific criterion prediction improves. Most likely beneficiaries: DA (currently 48.1% at score-5, lowest differentiation), CO (55.3% at score-5).

**Risk:** As with the bifactor g-head (v19b), adding a new objective creates capacity competition. Keep `--decorr-weight` small to start (0.01) and monitor per-dim held-out_r.

**Effort:** M (30 lines in distill.py + one training run)

---

### 2. Criterion Validity Rerun with v23 *(scientific)*

**What:** Re-run DonD and CMV criterion studies using the v23 model. Both scripts (`scripts/criterion_dealornodeal.py`, `scripts/criterion_validity_cmv.py`) auto-load from `models/psq-student/best.pt`, which is now v23. The criterion-validity-summary.md still cites v16 and v18 as the model used (held-out ~0.561–0.568 vs v23's 0.696).

**Why novel:** +0.13 improvement in held-out_r could materially change criterion AUC values and dimension rankings. CO in particular improved from ~0.45→0.549, which may now appear significant in CMV (persuasion) or DonD (deal), where it previously did not. New dimension rankings strengthen or refine the context-dependent primacy finding.

**No code changes required.** Zero code. Just run the scripts.

**Where:** `scripts/criterion_dealornodeal.py`, `scripts/criterion_validity_cmv.py` → update `criterion-validity-summary.md` and relevant distillation-research.md/journal.md sections.

**Effort:** M (2 runs ≈ 20–40 min compute + doc update)

---

### 3. Error Analysis for v23 by Source and Length *(scientific)*

**What:** Run `scripts/error_analysis.py --checkpoint models/psq-v23/best.pt`. Never run for any model newer than ~v2d (the script header still says "v2d"). Will reveal: which of the 11 source datasets the model systematically under-performs on, whether errors cluster by text length (128-token truncation effect), and which dimensions have the worst systematic bias.

**Why novel/actionable:** Directly identifies the next labeling batch target without guessing. If dreaddit texts have the highest error on RC, that's the next batch. If texts >90 tokens have 2× the error of shorter texts, that's the case for a 256-token experiment.

**Where:** `scripts/error_analysis.py --checkpoint models/psq-v23/best.pt --split all`

**Effort:** S (15 min to run, 30 min to interpret)

---

### 4. Turn-by-Turn Temporal PSQ *(conceptual + scientific)*

**What:** Score each conversation turn individually in CGA-Wiki rather than scoring the full conversation. Compute cross-lagged correlations between dimension trajectories: specifically, does AD decline before HI rises in derailing conversations?

**Why novel:** This is prediction T2 from `journal.md §24` and is listed as a pending test in `criterion-validity-summary.md §6b`. No prior tool tracks psychoemotional safety as a real-time trajectory. This would be the first published turn-by-turn safety dynamics analysis.

**Three sub-analyses:**
1. AD-as-leading-indicator: does AD at turn T predict HI at turn T+1 in derailing conversations?
2. Trajectory signature classification: are there recognizable PSQ trajectories (e.g., "slow escalation", "sudden collapse") that predict derailment?
3. Intervention point detection: at what turn does the PSQ profile diverge between derailing and non-derailing conversations?

**Where:** New script using CGA-Wiki conversation structure + `models/psq-student/` inference

**Effort:** L (2–4 hours for new script + analysis)

---

### 5. DonD Points-Scored Analysis *(scientific)*

**What:** Extend `scripts/criterion_dealornodeal.py` to test whether AD predicts deal (behavioral/relational outcome) but *not* points-scored (resource allocation/strategic outcome). This is prediction T3b from `journal.md §24`, currently untested.

**Why novel:** If AD predicts deal but not points, it confirms AD measures relational safety (whether parties stay cooperative) rather than strategic effectiveness (whether parties extract value). This sharpens the construct's theoretical interpretation significantly.

**Where:** Add ~20 lines to `scripts/criterion_dealornodeal.py` — the points data is already in the DonD dataset, just not currently analyzed.

**Effort:** S (20 lines of code + one run)

---

### 6. DA Computational Proxy *(scientific)*

**What:** Run a held-out partial correlation: does DA predict any criterion outcome variance beyond what TC+RC together explain? If the R² increment is <1% across all 4 criterion studies, that's computational evidence for DA deprecation — more actionable than waiting for expert validation.

**DA profile:** DA correlates 0.825 with TC and 0.768 with RC (sep-llm data). 48.1% of DA scores are exactly 5.0 (worst differentiation). DA held-out_r=0.608 (2nd weakest after CO).

**Decision rule:** If DA partial r < 0.05 after controlling TC+RC in all 4 criterion outcomes → recommend deprecation for expert validation to confirm. If partial r > 0.15 → retain with confidence.

**Where:** New 20-line script using held-out predictions from `eval_held_out.py` + v23 criterion study scores

**Effort:** S (20 lines + interpretation)

---

### 7. PSQ Profile Clustering / Safety Typology *(conceptual)*

**What:** With 3,065 fully labeled texts (all 10 dims, sep-llm), run k-means or Gaussian mixture model (k=4–6) on the 10-dim score vectors. Yield a PSQ archetypes typology: interpretable safety profiles (e.g., "high-hostility low-regulatory", "structurally safe but energetically depleted", "contractually clear but emotionally volatile").

**Why novel:** First published typology of psychoemotional safety profiles in natural text. Useful for the publication's discussion section and for practical deployment ("which category does this text fall into?").

**Where:** New script using `SELECT * FROM scores WHERE scorer='claude-sonnet-4-6'` grouped by text_id + sklearn clustering

**Effort:** M (30 min script + analysis)

---

### 8. Context-Aware Composite API *(conceptual + applied)*

**What:** Implement a small module that takes a 10-dim PSQ prediction and a context flag and returns a context-appropriate weighted composite. The research is done: criterion studies established which dimensions dominate in which contexts. The implementation is missing.

| Context | Key dimensions | Basis |
|---|---|---|
| Content moderation | AD, HI, DA | CGA-Wiki: AD strongest, HI/DA in multivariate |
| Persuasion quality | DA, CO, TC | CMV: DA top, CO/TC multivariate |
| Negotiation | AD, DA, HI | CaSiNo: AD/DA top, HI significant |
| Sustained engagement | ED, RB, RC | DonD: ED top (d=+0.614), RB 2nd |
| Therapeutic quality | RC, RB, CO, ED | Internal resources + contractual clarity |

**Why novel:** No existing tool provides context-sensitive psychometric weighting. This operationalizes the context-dependent primacy finding directly.

**Where:** New `scripts/psq_api.py` or as a JSON config `models/psq-student/context_weights.json`

**Effort:** M (30 min for module + config)

---

### 9. 128-Token → 256/512 Context Experiment *(architectural)*

**What:** Change `max_length` in `distill.py` from 128 to 256 and retrain. Many texts in the criterion datasets (DonD multi-turn dialogues, CGA-Wiki conversations) exceed 128 tokens and get truncated. The criterion-validity-summary.md explicitly cites this as limitation §6a.

**Expected impact:** Mostly affects long texts. If the model's worst errors (from #3 above) cluster in texts >90 tokens, this directly addresses them.

**Risk:** DistilBERT has absolute position embeddings up to 512. 256 tokens is safe. Training time roughly doubles. Held-out set was scored at 128 tokens, so comparison requires re-scoring held-out.

**Where:** `scripts/distill.py` → `--max-length` flag (already exists; just change the default) + new training run

**Effort:** L (training run + held-out re-evaluation)

---

---

## Error Analysis Results (v23, run 2026-02-28)

`python scripts/error_analysis.py --checkpoint models/psq-v23/best.pt --split all`
Full output saved to: `models/psq-student/error_analysis.json`

### Source-Level Performance (ranked by MAE, worst first)

| Source | n_scores | MAE | Bias | Notes |
|---|---|---|---|---|
| **berkeley** | 4,019 | **2.549** | **−2.259** | Worst by far. Short hate-speech texts. Model predicts safety when text is extremely threatening. |
| **ucc** | 8,976 | **2.296** | **−1.463** | Short hostile political comments. Model under-predicts TE/HI/AD/TC across the board. |
| civil_comments | 2,031 | 1.681 | −0.968 | Still problematic despite TE proxy removal. |
| dreaddit | 2,008 | 1.545 | +0.163 | Over-predicts slightly (predicts more safety depletion than actual). |
| prosocial | 2,018 | 1.251 | +0.024 | Near-neutral bias. |
| synthetic | 11,261 | 1.088 | −0.037 | Well-calibrated. |
| empathetic_dialogues | 3,947 | 0.942 | +0.180 | Good. |
| goemotions | 14,069 | 0.939 | +0.415 | Good, slight positive bias. |
| casino | 436 | 0.911 | +0.788 | Over-predicts (predicts higher safety than actual for negotiation texts). |
| esconv | 1,318 | 0.847 | +0.047 | Near-perfect. |
| claude_code | 1,013 | 0.806 | +0.149 | Near-perfect. |
| relabeled | 941 | 0.782 | −0.291 | Near-perfect. |
| politeness_wikipedia | 837 | 0.704 | +0.312 | Best sources. |
| **politeness_stack-exchange** | 1,193 | **0.615** | +0.314 | Best source — model excels on structured polite text. |

### Dimension-Level Findings

| Dim | MAE | r (vs proxy) | Bias | Compression | Key issue |
|---|---|---|---|---|---|
| TE | 2.234 | 0.109 | −1.228 | 0.80 | r=0.109 reflects proxy paradox, not real perf. Held-out r=0.800. |
| TC | 1.893 | 0.355 | −1.159 | 0.74 | Systematic under-prediction, UCC/berkeley dominated. |
| **AD** | 1.585 | 0.491 | −0.783 | **0.63** | Most compressed output. Predicts narrow range vs full actual range. |
| CO | 0.920 | **0.845** | −0.003 | 1.155 | r=0.845 against proxy (not adversarial). Held-out=0.549 — proxy CO ≠ LLM CO. |
| RC | 0.875 | 0.536 | +0.113 | 0.942 | Best dim by MAE, neutral bias. |

### Key Insights

**Berkeley and UCC are structural blind spots.** These sources contain short, cryptic hostile social media texts ("Your ID says it all, liar." → pred HI=1.9, actual=10.0). The model learned from longer, more emotionally explicit texts (dreaddit, empathetic_dialogues, esconv) and doesn't generalize to compressed hate-speech / political hostility. This is a **distribution mismatch problem**, not a token-length problem — these texts are short.

**AD output range is severely compressed (ratio=0.63).** The model predicts AD in a narrow band (std=1.54) while actuals span std=2.46. This explains why AD scored 46.9% at exactly 5.0 in the sep-llm data. The model has learned to be conservative with AD. More extreme AD examples needed.

**CO proxy vs LLM paradox.** CO r=0.845 against composite proxy labels (high!) but held-out r=0.549 against LLM labels (low). The CO proxy captures something, but it's a different something than what LLM labeling measures. CO is measuring two different constructs depending on label source.

**Actionable next batch target:** Berkeley texts with high HI/TE (extreme hostile language). Add 100–150 berkeley texts to a new labeling batch, score all 10 dims, particularly focus on AD and TC which are most compressed and biased on this source. Also include 50 UCC texts for the same reason.

---

## Criterion Study Results (v23 reruns, 2026-02-28)

### CMV v23 Results
- **AUC=0.5735** (was 0.590 v16) — slight regression within noise, result is cleaner
- DA still top predictor (r_pb=+0.059***)
- **CO not significant** (p=0.155) — confirms CO is not a persuasion predictor
- 7/10 dims significant (HI, AD, ED, RB, TC, CC, DA)
- TE near-zero (p=0.914) — v16's TE significance was adversarial proxy artifact

### DonD v23 Results
- **AUC=0.732** (was 0.686 v18) — **+0.046 improvement**, new best criterion validity result
- **5-fold CV: 0.723±0.010**
- **TE is now top predictor** (d=+0.801) — TE held-out improved 0.492→0.800 with v23
- ED is 7th bivariate but partial r (0.209) ≈ TE partial r (0.203) after length control
- **AD now positive bivariate** (r_pb=+0.138) vs v18 negative (-0.026)
- Q4 vs Q1 deal rate: **88.5% vs 59.7% = +28.7pp** (was 15.9pp with v18)
- **T3b CONFIRMED**: AD predicts deal (+0.138) but negatively predicts points (−0.070)

### Implication for novelty-hunt item #1 (Criterion Rerun)
Full rerun complete. CMV and DonD both updated. criterion-validity-summary.md overhauled with v23 numbers.

---

## Execution Order

Based on information dependency:

1. **#3 first** (error analysis) — reveals which sources/lengths are blind spots; informs whether #2 (decorr loss) or #9 (longer context) addresses the right problem.
2. **#1 next** (criterion rerun with v23) — zero code, just run scripts; updates publication claims.
3. **#5 with #1** (DonD points analysis) — add 20 lines to the DonD script before running it.
4. **#6** (DA proxy) — 20 lines; resolves DA construct validity question computationally.
5. **#2** (contrastive loss) — after error analysis informs the architectural decision.
6. **#7** (clustering) — standalone; run after having v23 criterion results for context.
7. **#4** (turn-by-turn temporal) — largest effort; do after simpler items unblock publication work.
8. **#8** (context API) — implement once criterion results with v23 are finalized.

---

## Open Questions Answered by v23 (update lab-notebook.md)

- **Q3:** Does CO-targeted data improve CO from 0.504? **YES** — v23 CO=0.549 (+0.045).
- **Q4:** Is CC penalized by proxy removal? **NO** — v23 CC=0.739 (+0.020 vs v22a). Proxy removal net-positive for CC.
